# Pipeline ETL de Ã“rdenes - Data Engineering

Pipeline completo de ETL que extrae datos de Ã³rdenes desde una API, los transforma con pandas y dbt, y los carga a DuckDB. Orquestado con Apache Airflow en Docker.

## ðŸ—ï¸ Arquitectura

```
API (FastAPI) â†’ Airflow DAG â†’ ExtracciÃ³n (Python/pandas) â†’ Parquet â†’ 
dbt (DuckDB) â†’ Tablas AnalÃ­ticas
```

## ðŸ“ Estructura del Proyecto

```
proyecto-ordenes/
â”œâ”€â”€ api_ordenes.py              # API de Ã³rdenes (datos sintÃ©ticos)
â”œâ”€â”€ extraer_ordenes.py          # Script de extracciÃ³n standalone
â”œâ”€â”€ docker-compose.yml          # Airflow + Postgres
â”œâ”€â”€ Dockerfile                  # Imagen custom de Airflow
â”œâ”€â”€ requirements.txt            # Dependencias Python
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ dag_ordenes.py     # DAG principal de ETL
â”œâ”€â”€ ordenes_analytics/          # Proyecto dbt
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/
â”‚       â”‚   â””â”€â”€ stg_ordenes.sql
â”‚       â””â”€â”€ marts/
â”‚           â””â”€â”€ mart_resumen_ventas.sql
â”œâ”€â”€ data/                       # Archivos parquet generados
â””â”€â”€ tests/
    â””â”€â”€ test_extraer.py        # Tests unitarios
```

## ðŸš€ Inicio RÃ¡pido

### 1. Instalar dependencias

```bash
uv sync
```

### 2. Iniciar la API de Ã³rdenes

```bash
# En una terminal
uv run python api_ordenes.py
```

La API estarÃ¡ disponible en `http://localhost:8000`

### 3. Levantar Airflow con Docker

```bash
# Build de la imagen
docker-compose build

# Iniciar servicios
docker-compose up -d

# Verificar que estÃ©n corriendo
docker-compose ps
```

Acceder a Airflow UI: `http://localhost:8080`
- Usuario: `airflow`
- Password: `airflow`

### 4. Ejecutar el pipeline

**OpciÃ³n A: Desde Airflow UI**
1. Ir a `http://localhost:8080`
2. Activar el DAG `etl_ordenes_diario`
3. Click en "Trigger DAG"

**OpciÃ³n B: Desde terminal**
```bash
docker-compose exec airflow-scheduler airflow dags test etl_ordenes_diario 2024-11-15
```

**OpciÃ³n C: Script standalone (sin Airflow)**
```bash
uv run python extraer_ordenes.py
```

## ðŸ”„ Flujo del Pipeline

### DAG de Airflow: `etl_ordenes_diario`

**Task 1: `extraer_ordenes`**
- Consulta API: `GET /orders?fecha={execution_date}`
- Procesa con pandas (transformaciones)
- Filtra Ã³rdenes `delivered`
- Guarda en: `data/ordenes_YYYY-MM-DD.parquet`

**Task 2: `ejecutar_dbt`**
- Lee parquets desde `data/`
- Crea tabla staging: `stg_ordenes`
- Crea tabla marts: `mart_resumen_ventas`
- Almacena en DuckDB: `ordenes_analytics/ordenes.duckdb`

### Transformaciones aplicadas

En **pandas**:
- ConversiÃ³n de fecha a datetime
- CÃ¡lculo de porcentaje de descuento
- Precio promedio por Ã­tem
- Limpieza de direcciones vacÃ­as

En **dbt**:
- Staging: Filtrado y limpieza
- Marts: Agregaciones por categorÃ­a

## ðŸ§ª Tests

```bash
# Ejecutar tests unitarios
uv run pytest tests/ -v

# Tests especÃ­ficos
uv run pytest tests/test_extraer.py::test_aplicar_transformaciones -v
```

Tests incluidos:
- âœ… Procesamiento de Ã³rdenes
- âœ… Transformaciones de datos
- âœ… CÃ¡lculo de mÃ©tricas
- âœ… Estructura de DataFrames

## ðŸ“Š Ver los Datos

### DuckDB CLI

```bash
# Abrir la base de datos
duckdb ordenes_analytics/ordenes.duckdb

# Dentro de DuckDB
SHOW TABLES;
SELECT * FROM stg_ordenes LIMIT 5;
SELECT * FROM mart_resumen_ventas;
.quit
```

### Con Python

```bash
cat > ver_datos.py << 'EOF'
import duckdb
con = duckdb.connect('ordenes_analytics/ordenes.duckdb')
print(con.execute("SELECT * FROM mart_resumen_ventas").df())
con.close()
EOF

uv run python ver_datos.py
```

### DBeaver (UI GrÃ¡fica)

1. Descargar DBeaver: https://dbeaver.io/download/
2. Nueva conexiÃ³n â†’ DuckDB
3. Path: `ordenes_analytics/ordenes.duckdb`
4. Explorar tablas visualmente

## ðŸ”§ ConfiguraciÃ³n

### Variables de entorno (opcional)

Crear `.env` en la raÃ­z:
```bash
API_BASE_URL=http://localhost:8000
AIRFLOW_UID=50000
```

### ConfiguraciÃ³n del DAG

En `airflow/dags/dag_ordenes.py`:
- **Schedule**: `0 2 * * *` (diario a las 2 AM)
- **Start date**: `2024-11-01`
- **Catchup**: `True` (ejecuta fechas pasadas si falla)

## ðŸ“ API de Ã“rdenes

La API genera datos sintÃ©ticos reproducibles (misma fecha = mismos datos).

**Endpoints:**
```bash
# RaÃ­z
GET http://localhost:8000/

# Obtener Ã³rdenes por fecha
GET http://localhost:8000/orders?fecha=2024-11-15
```

**CaracterÃ­sticas:**
- Genera 3-10 Ã³rdenes por dÃ­a
- 8 productos diferentes
- 5 estados posibles
- Datos reproducibles (usa fecha como seed)

## ðŸ³ Docker

### Comandos Ãºtiles

```bash
# Ver logs
docker-compose logs -f airflow-scheduler

# Reiniciar servicios
docker-compose restart

# Parar todo
docker-compose down

# Limpiar todo (incluye volÃºmenes)
docker-compose down -v
```

### Rebuild despuÃ©s de cambios

```bash
docker-compose build --no-cache
docker-compose up -d
```

## ðŸ› ï¸ TecnologÃ­as

- **Python 3.11**: Lenguaje principal
- **pandas**: TransformaciÃ³n de datos
- **FastAPI**: API de Ã³rdenes
- **Apache Airflow 2.8**: OrquestaciÃ³n
- **dbt 1.7**: Transformaciones SQL
- **DuckDB**: Data Warehouse OLAP
- **Docker**: ContainerizaciÃ³n
- **pytest**: Testing

## ðŸ“‚ Datos Generados

### Archivos Parquet
- UbicaciÃ³n: `data/ordenes_YYYY-MM-DD.parquet`
- Formato: Parquet (columnar)
- Contiene: Ã“rdenes delivered con transformaciones

### Tablas en DuckDB

**stg_ordenes**
- Ã“rdenes filtradas y limpias
- Todas las columnas originales + transformaciones
- Materializadas como tabla

**mart_resumen_ventas**
- Agregaciones por categorÃ­a
- MÃ©tricas: cantidad, totales, promedios
- Optimizada para anÃ¡lisis

## ðŸ” Troubleshooting

**Error: API no responde**
```bash
# Verificar que la API estÃ© corriendo
curl http://localhost:8000/
```

**Error: DAG no aparece en Airflow**
```bash
# Ver logs del scheduler
docker-compose logs -f airflow-scheduler

# Verificar que el archivo estÃ© en airflow/dags/
ls -la airflow/dags/dag_ordenes.py
```

**Error: dbt profile not found**
```bash
# Verificar que profiles.yml exista
cat ordenes_analytics/profiles.yml
```

**Error: No files found (parquet)**
- Verificar que la ruta en `stg_ordenes.sql` sea correcta
- Debe ser: `/opt/airflow/data/ordenes_*.parquet`

## ðŸŽ¯ PrÃ³ximos Pasos

Para completar el TP:
1. âœ… Pipeline ETL funcionando
2. âœ… Tests unitarios
3. âœ… Docker + Airflow
4. â¬œ GitHub Actions (CI/CD)
5. â¬œ ConexiÃ³n a Redshift
6. â¬œ DocumentaciÃ³n final

## ðŸ“„ Licencia

Proyecto educativo - ITBA Cloud Data Engineering