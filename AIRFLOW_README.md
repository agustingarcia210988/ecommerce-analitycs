# Airflow DAG - ETL de Ã“rdenes

## DescripciÃ³n del DAG

**Nombre**: `etl_ordenes_diario`

Este DAG ejecuta un pipeline ETL completo que:
1. Extrae Ã³rdenes de la API para la fecha de ejecuciÃ³n
2. Aplica transformaciones con pandas
3. Guarda en formato parquet
4. Ejecuta modelos dbt para crear tablas analÃ­ticas

## ConfiguraciÃ³n

```python
schedule_interval='0 2 * * *'  # Corre todos los dÃ­as a las 2 AM
start_date=datetime(2024, 11, 1)
catchup=True  # âœ… Ejecuta fechas pasadas
```

## CaracterÃ­sticas Importantes

### ðŸ”„ Catchup = True
- Si el DAG falla un dÃ­a, al correrlo despuÃ©s ejecuta TODAS las fechas que no se procesaron
- Ejemplo: Si falla el 15/11 y lo corres el 18/11, ejecutarÃ¡ el 15, 16, 17 y 18
- Usa `context['ds']` para obtener la fecha de cada ejecuciÃ³n

### ðŸ“… Fecha de EjecuciÃ³n
El DAG usa `context['ds']` (execution date) que Airflow pasa automÃ¡ticamente:
- Formato: `YYYY-MM-DD`
- Es la fecha lÃ³gica de ejecuciÃ³n, no la fecha actual
- Permite procesar fechas pasadas si es necesario

### ðŸ”— Dependencias entre Tasks
```
extraer_ordenes >> ejecutar_dbt
```
- Primero extrae datos de la API
- Luego ejecuta dbt para transformar

## Tasks

### Task 1: extraer_ordenes
- Consulta la API con la fecha de ejecuciÃ³n
- Procesa y transforma datos con pandas
- Guarda en `data/ordenes_YYYY-MM-DD.parquet`
- Si falla, reintenta 1 vez despuÃ©s de 5 minutos

### Task 2: ejecutar_dbt
- Ejecuta `dbt run` 
- Lee todos los parquets en data/
- Crea tablas staging y marts en DuckDB
- Muestra resumen de resultados

## Acceso a la API desde Docker

âš ï¸ **Importante**: Para que Docker acceda a la API en localhost:

```python
url = f"http://ecommerce-haze-8597.fly.dev///orders?fecha={fecha_ejecucion}"
```

`host.docker.internal` permite al contenedor acceder al localhost del host.

## CÃ³mo Probar

### 1. Backfill manual (ejecutar fechas pasadas)
```bash
docker-compose exec airflow-scheduler airflow dags backfill \
    -s 2024-11-01 \
    -e 2024-11-15 \
    etl_ordenes_diario
```

### 2. Ejecutar para una fecha especÃ­fica
```bash
docker-compose exec airflow-scheduler airflow dags test \
    etl_ordenes_diario 2024-11-15
```

### 3. Trigger manual desde UI
- Ir a http://localhost:8080
- Click en el DAG `etl_ordenes_diario`
- Click en "Trigger DAG"

## Monitoreo

### Ver logs
```bash
# Logs del scheduler
docker-compose logs -f airflow-scheduler

# Logs de una task especÃ­fica (desde Airflow UI)
# Click en el DAG â†’ Grid â†’ Click en task â†’ Logs
```

### Verificar datos generados
```bash
# Ver parquets generados
ls -la data/

# Verificar tablas en DuckDB
python ver_tablas.py
```

## Troubleshooting

### Error: No se puede conectar a la API
- VerificÃ¡ que la API estÃ© corriendo en el host
- Asegurate de usar `host.docker.internal` en la URL

### Error: dbt command not found
- NecesitÃ¡s instalar dbt en la imagen de Airflow
- Ver secciÃ³n "Personalizar Imagen"

### DAG no aparece en Airflow
- VerificÃ¡ que el archivo estÃ© en `airflow/dags/`
- EsperÃ¡ 1-2 minutos (Airflow escanea cada 30 segundos)
- RevisÃ¡ logs: `docker-compose logs airflow-scheduler`

## Personalizar Imagen de Airflow

Si necesitÃ¡s instalar dependencias (dbt, pandas, etc.):

**Crear `Dockerfile.airflow`**:
```dockerfile
FROM apache/airflow:2.8.1

USER root
RUN apt-get update && apt-get install -y git

USER airflow
COPY requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt
```

**Crear `requirements.txt`**:
```
dbt-duckdb>=1.7.0
pandas>=2.0.0
requests>=2.31.0
pyarrow>=14.0.0
duckdb>=0.9.0
```

**Actualizar `docker-compose.yml`**:
```yaml
# Cambiar:
image: apache/airflow:2.8.1

# Por:
build:
  context: .
  dockerfile: Dockerfile.airflow
```

Luego rebuild:
```bash
docker-compose build
docker-compose up -d
```
